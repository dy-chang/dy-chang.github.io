```python
import pandas as pd
import numpy as np
import missingno as msno
```


```python
#df = pd.read_csv('C:/Users/Translab PC/OneDrive - University of Missouri/Earthquake_MoDOT/2022/Survey/data_23.csv')
```


```python
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Residency</th>
      <th>H_number</th>
      <th>Pet_number</th>
      <th>Experience.0</th>
      <th>Experience.1</th>
      <th>Property_damage.0</th>
      <th>Property_damage.1</th>
      <th>Disruption.0</th>
      <th>Disruption.1</th>
      <th>Injury.0</th>
      <th>...</th>
      <th>H_evac_class</th>
      <th>Responsible</th>
      <th>Cost_evac_class</th>
      <th>Spend_evac_class</th>
      <th>Age_class</th>
      <th>Educate</th>
      <th>Dwelling.type</th>
      <th>House_ownership</th>
      <th>Employment</th>
      <th>Evac_decision</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>...</td>
      <td>6</td>
      <td>1</td>
      <td>3</td>
      <td>3</td>
      <td>0</td>
      <td>4</td>
      <td>2</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>55.0</td>
      <td>3</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>...</td>
      <td>6</td>
      <td>1</td>
      <td>3</td>
      <td>2</td>
      <td>3</td>
      <td>5</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1.0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>...</td>
      <td>2</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>3</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>8.0</td>
      <td>4</td>
      <td>5</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>...</td>
      <td>5</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>3</td>
      <td>5</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>10.0</td>
      <td>5</td>
      <td>4</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>...</td>
      <td>5</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>4</td>
      <td>0</td>
      <td>0</td>
      <td>6</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 43 columns</p>
</div>




```python
df.describe()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Residency</th>
      <th>H_number</th>
      <th>Pet_number</th>
      <th>Experience.0</th>
      <th>Experience.1</th>
      <th>Property_damage.0</th>
      <th>Property_damage.1</th>
      <th>Disruption.0</th>
      <th>Disruption.1</th>
      <th>Injury.0</th>
      <th>...</th>
      <th>H_evac_class</th>
      <th>Responsible</th>
      <th>Cost_evac_class</th>
      <th>Spend_evac_class</th>
      <th>Age_class</th>
      <th>Educate</th>
      <th>Dwelling.type</th>
      <th>House_ownership</th>
      <th>Employment</th>
      <th>Evac_decision</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>490.000000</td>
      <td>490.000000</td>
      <td>490.000000</td>
      <td>490.000000</td>
      <td>490.000000</td>
      <td>490.000000</td>
      <td>490.000000</td>
      <td>490.000000</td>
      <td>490.000000</td>
      <td>490.000000</td>
      <td>...</td>
      <td>490.000000</td>
      <td>490.000000</td>
      <td>490.000000</td>
      <td>490.000000</td>
      <td>490.000000</td>
      <td>490.000000</td>
      <td>490.000000</td>
      <td>490.000000</td>
      <td>490.000000</td>
      <td>490.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>28.609490</td>
      <td>2.910204</td>
      <td>2.089796</td>
      <td>0.181633</td>
      <td>0.818367</td>
      <td>0.961224</td>
      <td>0.038776</td>
      <td>0.932653</td>
      <td>0.067347</td>
      <td>0.995918</td>
      <td>...</td>
      <td>3.189796</td>
      <td>0.734694</td>
      <td>1.869388</td>
      <td>1.114286</td>
      <td>2.136735</td>
      <td>2.326531</td>
      <td>0.263265</td>
      <td>0.228571</td>
      <td>0.667347</td>
      <td>0.253061</td>
    </tr>
    <tr>
      <th>std</th>
      <td>19.840123</td>
      <td>1.676280</td>
      <td>2.587546</td>
      <td>0.385935</td>
      <td>0.385935</td>
      <td>0.193257</td>
      <td>0.193257</td>
      <td>0.250878</td>
      <td>0.250878</td>
      <td>0.063822</td>
      <td>...</td>
      <td>2.317952</td>
      <td>0.691033</td>
      <td>1.251290</td>
      <td>1.118354</td>
      <td>1.499720</td>
      <td>2.218972</td>
      <td>0.656942</td>
      <td>0.457610</td>
      <td>1.348162</td>
      <td>0.435210</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.250000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>10.625000</td>
      <td>2.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>28.000000</td>
      <td>3.000000</td>
      <td>2.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>...</td>
      <td>4.000000</td>
      <td>1.000000</td>
      <td>2.000000</td>
      <td>1.000000</td>
      <td>2.000000</td>
      <td>4.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>41.000000</td>
      <td>4.000000</td>
      <td>3.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>...</td>
      <td>5.000000</td>
      <td>1.000000</td>
      <td>3.000000</td>
      <td>1.000000</td>
      <td>3.000000</td>
      <td>4.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>103.000000</td>
      <td>15.000000</td>
      <td>25.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>...</td>
      <td>6.000000</td>
      <td>3.000000</td>
      <td>5.000000</td>
      <td>4.000000</td>
      <td>5.000000</td>
      <td>5.000000</td>
      <td>2.000000</td>
      <td>2.000000</td>
      <td>6.000000</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
<p>8 rows × 43 columns</p>
</div>




```python
print(df.shape)
```

    (490, 43)
    


```python
X=df.iloc[:,:-1]
y=df['Evac_decision']
```


```python
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
```


```python
# Feature selection

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
```


```python
### Apply SelectKBest Algorithm
ordered_rank_features=SelectKBest(score_func=chi2,k=42)
ordered_feature=ordered_rank_features.fit(X,y)
```


```python
dfscores=pd.DataFrame(ordered_feature.scores_,columns=["Score"])
dfcolumns=pd.DataFrame(X.columns)
```


```python
features_rank=pd.concat([dfcolumns,dfscores],axis=1)
```


```python
features_rank.columns=['Features','Score']
features_rank
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Features</th>
      <th>Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Residency</td>
      <td>11.980783</td>
    </tr>
    <tr>
      <th>1</th>
      <td>H_number</td>
      <td>0.741212</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Pet_number</td>
      <td>1.183414</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Experience.0</td>
      <td>1.191737</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Experience.1</td>
      <td>0.264500</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Property_damage.0</td>
      <td>0.007336</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Property_damage.1</td>
      <td>0.181859</td>
    </tr>
    <tr>
      <th>7</th>
      <td>Disruption.0</td>
      <td>0.001426</td>
    </tr>
    <tr>
      <th>8</th>
      <td>Disruption.1</td>
      <td>0.019753</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Injury.0</td>
      <td>0.002777</td>
    </tr>
    <tr>
      <th>10</th>
      <td>Injury.1</td>
      <td>0.677596</td>
    </tr>
    <tr>
      <th>11</th>
      <td>Pet_Dog.0</td>
      <td>0.118520</td>
    </tr>
    <tr>
      <th>12</th>
      <td>Pet_Dog.1</td>
      <td>0.080366</td>
    </tr>
    <tr>
      <th>13</th>
      <td>Pet_Cat.0</td>
      <td>0.302873</td>
    </tr>
    <tr>
      <th>14</th>
      <td>Pet_Cat.1</td>
      <td>0.565008</td>
    </tr>
    <tr>
      <th>15</th>
      <td>Pet_Others.0</td>
      <td>0.155126</td>
    </tr>
    <tr>
      <th>16</th>
      <td>Pet_Others.1</td>
      <td>1.534021</td>
    </tr>
    <tr>
      <th>17</th>
      <td>Gender.0</td>
      <td>0.863588</td>
    </tr>
    <tr>
      <th>18</th>
      <td>Gender.1</td>
      <td>1.670293</td>
    </tr>
    <tr>
      <th>19</th>
      <td>Hearing.0</td>
      <td>0.053532</td>
    </tr>
    <tr>
      <th>20</th>
      <td>Hearing.1</td>
      <td>0.792617</td>
    </tr>
    <tr>
      <th>21</th>
      <td>Vision.0</td>
      <td>0.346919</td>
    </tr>
    <tr>
      <th>22</th>
      <td>Vision.1</td>
      <td>2.583952</td>
    </tr>
    <tr>
      <th>23</th>
      <td>Concentrating.0</td>
      <td>0.000134</td>
    </tr>
    <tr>
      <th>24</th>
      <td>Concentrating.1</td>
      <td>0.001268</td>
    </tr>
    <tr>
      <th>25</th>
      <td>Walking.0</td>
      <td>0.615526</td>
    </tr>
    <tr>
      <th>26</th>
      <td>Walking.1</td>
      <td>2.593068</td>
    </tr>
    <tr>
      <th>27</th>
      <td>Dressing.0</td>
      <td>0.013498</td>
    </tr>
    <tr>
      <th>28</th>
      <td>Dressing.1</td>
      <td>0.222722</td>
    </tr>
    <tr>
      <th>29</th>
      <td>Doctor.0</td>
      <td>0.022896</td>
    </tr>
    <tr>
      <th>30</th>
      <td>Doctor.1</td>
      <td>0.226414</td>
    </tr>
    <tr>
      <th>31</th>
      <td>Expectation</td>
      <td>0.178860</td>
    </tr>
    <tr>
      <th>32</th>
      <td>H_normal_class</td>
      <td>6.698754</td>
    </tr>
    <tr>
      <th>33</th>
      <td>H_evac_class</td>
      <td>13.207121</td>
    </tr>
    <tr>
      <th>34</th>
      <td>Responsible</td>
      <td>0.916678</td>
    </tr>
    <tr>
      <th>35</th>
      <td>Cost_evac_class</td>
      <td>3.107542</td>
    </tr>
    <tr>
      <th>36</th>
      <td>Spend_evac_class</td>
      <td>1.852897</td>
    </tr>
    <tr>
      <th>37</th>
      <td>Age_class</td>
      <td>6.532264</td>
    </tr>
    <tr>
      <th>38</th>
      <td>Educate</td>
      <td>0.844492</td>
    </tr>
    <tr>
      <th>39</th>
      <td>Dwelling.type</td>
      <td>5.287882</td>
    </tr>
    <tr>
      <th>40</th>
      <td>House_ownership</td>
      <td>1.511700</td>
    </tr>
    <tr>
      <th>41</th>
      <td>Employment</td>
      <td>0.049605</td>
    </tr>
  </tbody>
</table>
</div>




```python
features_rank.nlargest(10,'Score')
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Features</th>
      <th>Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>33</th>
      <td>H_evac_class</td>
      <td>13.207121</td>
    </tr>
    <tr>
      <th>0</th>
      <td>Residency</td>
      <td>11.980783</td>
    </tr>
    <tr>
      <th>32</th>
      <td>H_normal_class</td>
      <td>6.698754</td>
    </tr>
    <tr>
      <th>37</th>
      <td>Age_class</td>
      <td>6.532264</td>
    </tr>
    <tr>
      <th>39</th>
      <td>Dwelling.type</td>
      <td>5.287882</td>
    </tr>
    <tr>
      <th>35</th>
      <td>Cost_evac_class</td>
      <td>3.107542</td>
    </tr>
    <tr>
      <th>26</th>
      <td>Walking.1</td>
      <td>2.593068</td>
    </tr>
    <tr>
      <th>22</th>
      <td>Vision.1</td>
      <td>2.583952</td>
    </tr>
    <tr>
      <th>36</th>
      <td>Spend_evac_class</td>
      <td>1.852897</td>
    </tr>
    <tr>
      <th>18</th>
      <td>Gender.1</td>
      <td>1.670293</td>
    </tr>
  </tbody>
</table>
</div>




```python
from sklearn.ensemble import ExtraTreesClassifier
import matplotlib.pyplot as plt
model=ExtraTreesClassifier()
model.fit(X,y)
```

    C:\Users\Translab PC\Anaconda3\lib\site-packages\sklearn\ensemble\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.
      "10 in version 0.20 to 100 in 0.22.", FutureWarning)
    C:\Users\Translab PC\Anaconda3\lib\site-packages\sklearn\ensemble\forest.py:487: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      y_store_unique_indices = np.zeros(y.shape, dtype=np.int)
    C:\Users\Translab PC\Anaconda3\lib\site-packages\sklearn\tree\tree.py:149: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      y_encoded = np.zeros(y.shape, dtype=np.int)
    C:\Users\Translab PC\Anaconda3\lib\site-packages\sklearn\tree\tree.py:149: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      y_encoded = np.zeros(y.shape, dtype=np.int)
    C:\Users\Translab PC\Anaconda3\lib\site-packages\sklearn\tree\tree.py:149: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      y_encoded = np.zeros(y.shape, dtype=np.int)
    C:\Users\Translab PC\Anaconda3\lib\site-packages\sklearn\tree\tree.py:149: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      y_encoded = np.zeros(y.shape, dtype=np.int)
    C:\Users\Translab PC\Anaconda3\lib\site-packages\sklearn\tree\tree.py:149: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      y_encoded = np.zeros(y.shape, dtype=np.int)
    C:\Users\Translab PC\Anaconda3\lib\site-packages\sklearn\tree\tree.py:149: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      y_encoded = np.zeros(y.shape, dtype=np.int)
    C:\Users\Translab PC\Anaconda3\lib\site-packages\sklearn\tree\tree.py:149: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      y_encoded = np.zeros(y.shape, dtype=np.int)
    C:\Users\Translab PC\Anaconda3\lib\site-packages\sklearn\tree\tree.py:149: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      y_encoded = np.zeros(y.shape, dtype=np.int)
    C:\Users\Translab PC\Anaconda3\lib\site-packages\sklearn\tree\tree.py:149: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      y_encoded = np.zeros(y.shape, dtype=np.int)
    C:\Users\Translab PC\Anaconda3\lib\site-packages\sklearn\tree\tree.py:149: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      y_encoded = np.zeros(y.shape, dtype=np.int)
    




    ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',
               max_depth=None, max_features='auto', max_leaf_nodes=None,
               min_impurity_decrease=0.0, min_impurity_split=None,
               min_samples_leaf=1, min_samples_split=2,
               min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,
               oob_score=False, random_state=None, verbose=0, warm_start=False)




```python
print(model.feature_importances_)
```

    [0.08133133 0.05309964 0.05017442 0.02286128 0.02048161 0.00307565
     0.00378955 0.00972916 0.0118755  0.00047609 0.         0.02438566
     0.02741762 0.02939244 0.01439437 0.01358924 0.01028896 0.01308277
     0.01658551 0.01098793 0.00886238 0.01189661 0.01311016 0.00839478
     0.00689946 0.00995487 0.01074535 0.00431357 0.00420363 0.00781999
     0.00455152 0.01669331 0.06130481 0.07378964 0.03755655 0.05058289
     0.05400209 0.06491078 0.04047274 0.0264409  0.02640353 0.04007172]
    


```python
ranked_features=pd.Series(model.feature_importances_,index=X.columns)
ranked_features.nlargest(10).plot(kind='barh')
plt.show()
```


    
![png](output_15_0.png)
    



```python
#Correlation

df.corr()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Residency</th>
      <th>H_number</th>
      <th>Pet_number</th>
      <th>Experience.0</th>
      <th>Experience.1</th>
      <th>Property_damage.0</th>
      <th>Property_damage.1</th>
      <th>Disruption.0</th>
      <th>Disruption.1</th>
      <th>Injury.0</th>
      <th>...</th>
      <th>H_evac_class</th>
      <th>Responsible</th>
      <th>Cost_evac_class</th>
      <th>Spend_evac_class</th>
      <th>Age_class</th>
      <th>Educate</th>
      <th>Dwelling.type</th>
      <th>House_ownership</th>
      <th>Employment</th>
      <th>Evac_decision</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Residency</th>
      <td>1.000000</td>
      <td>-0.093712</td>
      <td>0.016825</td>
      <td>-0.033850</td>
      <td>0.033850</td>
      <td>0.000843</td>
      <td>-0.000843</td>
      <td>0.022643</td>
      <td>-0.022643</td>
      <td>0.016504</td>
      <td>...</td>
      <td>0.002302</td>
      <td>-0.121380</td>
      <td>-0.001037</td>
      <td>0.055481</td>
      <td>0.417846</td>
      <td>0.034905</td>
      <td>-0.159774</td>
      <td>-0.211393</td>
      <td>-0.014167</td>
      <td>-0.042199</td>
    </tr>
    <tr>
      <th>H_number</th>
      <td>-0.093712</td>
      <td>1.000000</td>
      <td>0.182908</td>
      <td>0.034746</td>
      <td>-0.034746</td>
      <td>-0.010770</td>
      <td>0.010770</td>
      <td>0.029355</td>
      <td>-0.029355</td>
      <td>-0.041663</td>
      <td>...</td>
      <td>0.048605</td>
      <td>0.018231</td>
      <td>0.045095</td>
      <td>0.018576</td>
      <td>-0.256226</td>
      <td>-0.050928</td>
      <td>-0.010058</td>
      <td>-0.018509</td>
      <td>-0.026818</td>
      <td>0.039622</td>
    </tr>
    <tr>
      <th>Pet_number</th>
      <td>0.016825</td>
      <td>0.182908</td>
      <td>1.000000</td>
      <td>0.024590</td>
      <td>-0.024590</td>
      <td>0.047872</td>
      <td>-0.047872</td>
      <td>0.015635</td>
      <td>-0.015635</td>
      <td>0.014607</td>
      <td>...</td>
      <td>-0.003870</td>
      <td>0.056811</td>
      <td>-0.000160</td>
      <td>-0.026167</td>
      <td>-0.060611</td>
      <td>0.012691</td>
      <td>-0.060854</td>
      <td>-0.098541</td>
      <td>0.113514</td>
      <td>-0.027484</td>
    </tr>
    <tr>
      <th>Experience.0</th>
      <td>-0.033850</td>
      <td>0.034746</td>
      <td>0.024590</td>
      <td>1.000000</td>
      <td>-1.000000</td>
      <td>0.012366</td>
      <td>-0.012366</td>
      <td>0.042113</td>
      <td>-0.042113</td>
      <td>0.030160</td>
      <td>...</td>
      <td>-0.022613</td>
      <td>0.043034</td>
      <td>0.049226</td>
      <td>-0.019764</td>
      <td>-0.025331</td>
      <td>-0.145811</td>
      <td>-0.068000</td>
      <td>0.019188</td>
      <td>-0.005478</td>
      <td>0.054515</td>
    </tr>
    <tr>
      <th>Experience.1</th>
      <td>0.033850</td>
      <td>-0.034746</td>
      <td>-0.024590</td>
      <td>-1.000000</td>
      <td>1.000000</td>
      <td>-0.012366</td>
      <td>0.012366</td>
      <td>-0.042113</td>
      <td>0.042113</td>
      <td>-0.030160</td>
      <td>...</td>
      <td>0.022613</td>
      <td>-0.043034</td>
      <td>-0.049226</td>
      <td>0.019764</td>
      <td>0.025331</td>
      <td>0.145811</td>
      <td>0.068000</td>
      <td>-0.019188</td>
      <td>0.005478</td>
      <td>-0.054515</td>
    </tr>
    <tr>
      <th>Property_damage.0</th>
      <td>0.000843</td>
      <td>-0.010770</td>
      <td>0.047872</td>
      <td>0.012366</td>
      <td>-0.012366</td>
      <td>1.000000</td>
      <td>-1.000000</td>
      <td>0.241280</td>
      <td>-0.241280</td>
      <td>0.318742</td>
      <td>...</td>
      <td>-0.015493</td>
      <td>0.014688</td>
      <td>-0.071726</td>
      <td>-0.074073</td>
      <td>-0.024004</td>
      <td>-0.103939</td>
      <td>0.048356</td>
      <td>0.031052</td>
      <td>0.052428</td>
      <td>0.019650</td>
    </tr>
    <tr>
      <th>Property_damage.1</th>
      <td>-0.000843</td>
      <td>0.010770</td>
      <td>-0.047872</td>
      <td>-0.012366</td>
      <td>0.012366</td>
      <td>-1.000000</td>
      <td>1.000000</td>
      <td>-0.241280</td>
      <td>0.241280</td>
      <td>-0.318742</td>
      <td>...</td>
      <td>0.015493</td>
      <td>-0.014688</td>
      <td>0.071726</td>
      <td>0.074073</td>
      <td>0.024004</td>
      <td>0.103939</td>
      <td>-0.048356</td>
      <td>-0.031052</td>
      <td>-0.052428</td>
      <td>-0.019650</td>
    </tr>
    <tr>
      <th>Disruption.0</th>
      <td>0.022643</td>
      <td>0.029355</td>
      <td>0.015635</td>
      <td>0.042113</td>
      <td>-0.042113</td>
      <td>0.241280</td>
      <td>-0.241280</td>
      <td>1.000000</td>
      <td>-1.000000</td>
      <td>0.238235</td>
      <td>...</td>
      <td>-0.048307</td>
      <td>0.038276</td>
      <td>-0.028078</td>
      <td>-0.030821</td>
      <td>0.008219</td>
      <td>-0.055927</td>
      <td>0.045758</td>
      <td>0.027483</td>
      <td>0.036413</td>
      <td>0.006574</td>
    </tr>
    <tr>
      <th>Disruption.1</th>
      <td>-0.022643</td>
      <td>-0.029355</td>
      <td>-0.015635</td>
      <td>-0.042113</td>
      <td>0.042113</td>
      <td>-0.241280</td>
      <td>0.241280</td>
      <td>-1.000000</td>
      <td>1.000000</td>
      <td>-0.238235</td>
      <td>...</td>
      <td>0.048307</td>
      <td>-0.038276</td>
      <td>0.028078</td>
      <td>0.030821</td>
      <td>-0.008219</td>
      <td>0.055927</td>
      <td>-0.045758</td>
      <td>-0.027483</td>
      <td>-0.036413</td>
      <td>-0.006574</td>
    </tr>
    <tr>
      <th>Injury.0</th>
      <td>0.016504</td>
      <td>-0.041663</td>
      <td>0.014607</td>
      <td>0.030160</td>
      <td>-0.030160</td>
      <td>0.318742</td>
      <td>-0.318742</td>
      <td>0.238235</td>
      <td>-0.238235</td>
      <td>1.000000</td>
      <td>...</td>
      <td>-0.050046</td>
      <td>-0.070972</td>
      <td>-0.083511</td>
      <td>-0.108055</td>
      <td>0.027208</td>
      <td>-0.062770</td>
      <td>0.025681</td>
      <td>0.032009</td>
      <td>-0.063347</td>
      <td>0.037263</td>
    </tr>
    <tr>
      <th>Injury.1</th>
      <td>-0.016504</td>
      <td>0.041663</td>
      <td>-0.014607</td>
      <td>-0.030160</td>
      <td>0.030160</td>
      <td>-0.318742</td>
      <td>0.318742</td>
      <td>-0.238235</td>
      <td>0.238235</td>
      <td>-1.000000</td>
      <td>...</td>
      <td>0.050046</td>
      <td>0.070972</td>
      <td>0.083511</td>
      <td>0.108055</td>
      <td>-0.027208</td>
      <td>0.062770</td>
      <td>-0.025681</td>
      <td>-0.032009</td>
      <td>0.063347</td>
      <td>-0.037263</td>
    </tr>
    <tr>
      <th>Pet_Dog.0</th>
      <td>0.045024</td>
      <td>-0.181847</td>
      <td>-0.324645</td>
      <td>-0.053539</td>
      <td>0.053539</td>
      <td>0.036138</td>
      <td>-0.036138</td>
      <td>-0.011040</td>
      <td>0.011040</td>
      <td>-0.012514</td>
      <td>...</td>
      <td>-0.035166</td>
      <td>0.081515</td>
      <td>0.019501</td>
      <td>-0.017230</td>
      <td>0.113609</td>
      <td>0.019412</td>
      <td>0.081581</td>
      <td>0.134124</td>
      <td>-0.012768</td>
      <td>-0.020147</td>
    </tr>
    <tr>
      <th>Pet_Dog.1</th>
      <td>-0.045024</td>
      <td>0.181847</td>
      <td>0.324645</td>
      <td>0.053539</td>
      <td>-0.053539</td>
      <td>-0.036138</td>
      <td>0.036138</td>
      <td>0.011040</td>
      <td>-0.011040</td>
      <td>0.012514</td>
      <td>...</td>
      <td>0.035166</td>
      <td>-0.081515</td>
      <td>-0.019501</td>
      <td>0.017230</td>
      <td>-0.113609</td>
      <td>-0.019412</td>
      <td>-0.081581</td>
      <td>-0.134124</td>
      <td>0.012768</td>
      <td>0.020147</td>
    </tr>
    <tr>
      <th>Pet_Cat.0</th>
      <td>0.024265</td>
      <td>-0.018806</td>
      <td>-0.391976</td>
      <td>0.000657</td>
      <td>-0.000657</td>
      <td>-0.058341</td>
      <td>0.058341</td>
      <td>-0.008821</td>
      <td>0.008821</td>
      <td>0.020284</td>
      <td>...</td>
      <td>-0.034290</td>
      <td>-0.008481</td>
      <td>-0.007997</td>
      <td>0.017410</td>
      <td>0.041101</td>
      <td>-0.054398</td>
      <td>0.026216</td>
      <td>0.075731</td>
      <td>0.003549</td>
      <td>0.042085</td>
    </tr>
    <tr>
      <th>Pet_Cat.1</th>
      <td>-0.024265</td>
      <td>0.018806</td>
      <td>0.391976</td>
      <td>-0.000657</td>
      <td>0.000657</td>
      <td>0.058341</td>
      <td>-0.058341</td>
      <td>0.008821</td>
      <td>-0.008821</td>
      <td>-0.020284</td>
      <td>...</td>
      <td>0.034290</td>
      <td>0.008481</td>
      <td>0.007997</td>
      <td>-0.017410</td>
      <td>-0.041101</td>
      <td>0.054398</td>
      <td>-0.026216</td>
      <td>-0.075731</td>
      <td>-0.003549</td>
      <td>-0.042085</td>
    </tr>
    <tr>
      <th>Pet_Others.0</th>
      <td>0.094458</td>
      <td>-0.156312</td>
      <td>-0.371687</td>
      <td>0.021509</td>
      <td>-0.021509</td>
      <td>0.045941</td>
      <td>-0.045941</td>
      <td>-0.000863</td>
      <td>0.000863</td>
      <td>-0.020358</td>
      <td>...</td>
      <td>-0.013608</td>
      <td>0.010863</td>
      <td>0.011999</td>
      <td>-0.005422</td>
      <td>0.071474</td>
      <td>-0.007352</td>
      <td>0.019888</td>
      <td>-0.011042</td>
      <td>0.010655</td>
      <td>-0.058713</td>
    </tr>
    <tr>
      <th>Pet_Others.1</th>
      <td>-0.094458</td>
      <td>0.156312</td>
      <td>0.371687</td>
      <td>-0.021509</td>
      <td>0.021509</td>
      <td>-0.045941</td>
      <td>0.045941</td>
      <td>0.000863</td>
      <td>-0.000863</td>
      <td>0.020358</td>
      <td>...</td>
      <td>0.013608</td>
      <td>-0.010863</td>
      <td>-0.011999</td>
      <td>0.005422</td>
      <td>-0.071474</td>
      <td>0.007352</td>
      <td>-0.019888</td>
      <td>0.011042</td>
      <td>-0.010655</td>
      <td>0.058713</td>
    </tr>
    <tr>
      <th>Gender.0</th>
      <td>-0.170797</td>
      <td>0.041150</td>
      <td>0.049964</td>
      <td>0.014883</td>
      <td>-0.014883</td>
      <td>0.100906</td>
      <td>-0.100906</td>
      <td>-0.004242</td>
      <td>0.004242</td>
      <td>0.021500</td>
      <td>...</td>
      <td>-0.030316</td>
      <td>0.178968</td>
      <td>0.024758</td>
      <td>-0.126850</td>
      <td>-0.175784</td>
      <td>0.006858</td>
      <td>0.078502</td>
      <td>0.076964</td>
      <td>0.046187</td>
      <td>0.071911</td>
    </tr>
    <tr>
      <th>Gender.1</th>
      <td>0.170797</td>
      <td>-0.041150</td>
      <td>-0.049964</td>
      <td>-0.014883</td>
      <td>0.014883</td>
      <td>-0.100906</td>
      <td>0.100906</td>
      <td>0.004242</td>
      <td>-0.004242</td>
      <td>-0.021500</td>
      <td>...</td>
      <td>0.030316</td>
      <td>-0.178968</td>
      <td>-0.024758</td>
      <td>0.126850</td>
      <td>0.175784</td>
      <td>-0.006858</td>
      <td>-0.078502</td>
      <td>-0.076964</td>
      <td>-0.046187</td>
      <td>-0.071911</td>
    </tr>
    <tr>
      <th>Hearing.0</th>
      <td>-0.109552</td>
      <td>-0.053985</td>
      <td>-0.010431</td>
      <td>0.035456</td>
      <td>-0.035456</td>
      <td>-0.052196</td>
      <td>0.052196</td>
      <td>-0.002935</td>
      <td>0.002935</td>
      <td>-0.016637</td>
      <td>...</td>
      <td>-0.080069</td>
      <td>-0.027014</td>
      <td>-0.114340</td>
      <td>-0.018438</td>
      <td>-0.155341</td>
      <td>0.004245</td>
      <td>-0.036263</td>
      <td>0.074925</td>
      <td>-0.008168</td>
      <td>-0.041555</td>
    </tr>
    <tr>
      <th>Hearing.1</th>
      <td>0.109552</td>
      <td>0.053985</td>
      <td>0.010431</td>
      <td>-0.035456</td>
      <td>0.035456</td>
      <td>0.052196</td>
      <td>-0.052196</td>
      <td>0.002935</td>
      <td>-0.002935</td>
      <td>0.016637</td>
      <td>...</td>
      <td>0.080069</td>
      <td>0.027014</td>
      <td>0.114340</td>
      <td>0.018438</td>
      <td>0.155341</td>
      <td>-0.004245</td>
      <td>0.036263</td>
      <td>-0.074925</td>
      <td>0.008168</td>
      <td>0.041555</td>
    </tr>
    <tr>
      <th>Vision.0</th>
      <td>0.032464</td>
      <td>-0.159235</td>
      <td>-0.058147</td>
      <td>-0.056783</td>
      <td>0.056783</td>
      <td>0.057299</td>
      <td>-0.057299</td>
      <td>0.002366</td>
      <td>-0.002366</td>
      <td>-0.023457</td>
      <td>...</td>
      <td>0.021848</td>
      <td>-0.113366</td>
      <td>-0.083772</td>
      <td>-0.069957</td>
      <td>0.050308</td>
      <td>0.031175</td>
      <td>-0.064791</td>
      <td>-0.037905</td>
      <td>-0.076431</td>
      <td>-0.077339</td>
    </tr>
    <tr>
      <th>Vision.1</th>
      <td>-0.032464</td>
      <td>0.159235</td>
      <td>0.058147</td>
      <td>0.056783</td>
      <td>-0.056783</td>
      <td>-0.057299</td>
      <td>0.057299</td>
      <td>-0.002366</td>
      <td>0.002366</td>
      <td>0.023457</td>
      <td>...</td>
      <td>-0.021848</td>
      <td>0.113366</td>
      <td>0.083772</td>
      <td>0.069957</td>
      <td>-0.050308</td>
      <td>-0.031175</td>
      <td>0.064791</td>
      <td>0.037905</td>
      <td>0.076431</td>
      <td>0.077339</td>
    </tr>
    <tr>
      <th>Concentrating.0</th>
      <td>0.019195</td>
      <td>-0.195423</td>
      <td>-0.130780</td>
      <td>0.009648</td>
      <td>-0.009648</td>
      <td>-0.065420</td>
      <td>0.065420</td>
      <td>-0.087528</td>
      <td>0.087528</td>
      <td>-0.020852</td>
      <td>...</td>
      <td>0.071591</td>
      <td>-0.085025</td>
      <td>0.015863</td>
      <td>0.076742</td>
      <td>0.034353</td>
      <td>0.098002</td>
      <td>-0.027736</td>
      <td>-0.094858</td>
      <td>-0.126765</td>
      <td>-0.001692</td>
    </tr>
    <tr>
      <th>Concentrating.1</th>
      <td>-0.019195</td>
      <td>0.195423</td>
      <td>0.130780</td>
      <td>-0.009648</td>
      <td>0.009648</td>
      <td>0.065420</td>
      <td>-0.065420</td>
      <td>0.087528</td>
      <td>-0.087528</td>
      <td>0.020852</td>
      <td>...</td>
      <td>-0.071591</td>
      <td>0.085025</td>
      <td>-0.015863</td>
      <td>-0.076742</td>
      <td>-0.034353</td>
      <td>-0.098002</td>
      <td>0.027736</td>
      <td>0.094858</td>
      <td>0.126765</td>
      <td>0.001692</td>
    </tr>
    <tr>
      <th>Walking.0</th>
      <td>-0.144538</td>
      <td>-0.084934</td>
      <td>-0.033203</td>
      <td>0.041319</td>
      <td>-0.041319</td>
      <td>-0.044161</td>
      <td>0.044161</td>
      <td>-0.027518</td>
      <td>0.027518</td>
      <td>-0.031190</td>
      <td>...</td>
      <td>0.044411</td>
      <td>-0.067114</td>
      <td>0.085924</td>
      <td>0.100872</td>
      <td>-0.225380</td>
      <td>0.060077</td>
      <td>-0.065181</td>
      <td>-0.051183</td>
      <td>-0.332007</td>
      <td>0.080921</td>
    </tr>
    <tr>
      <th>Walking.1</th>
      <td>0.144538</td>
      <td>0.084934</td>
      <td>0.033203</td>
      <td>-0.041319</td>
      <td>0.041319</td>
      <td>0.044161</td>
      <td>-0.044161</td>
      <td>0.027518</td>
      <td>-0.027518</td>
      <td>0.031190</td>
      <td>...</td>
      <td>-0.044411</td>
      <td>0.067114</td>
      <td>-0.085924</td>
      <td>-0.100872</td>
      <td>0.225380</td>
      <td>-0.060077</td>
      <td>0.065181</td>
      <td>0.051183</td>
      <td>0.332007</td>
      <td>-0.080921</td>
    </tr>
    <tr>
      <th>Dressing.0</th>
      <td>-0.015275</td>
      <td>-0.254722</td>
      <td>-0.032265</td>
      <td>-0.020850</td>
      <td>0.020850</td>
      <td>-0.049445</td>
      <td>0.049445</td>
      <td>-0.031072</td>
      <td>0.031072</td>
      <td>-0.015760</td>
      <td>...</td>
      <td>0.016381</td>
      <td>-0.094613</td>
      <td>0.030546</td>
      <td>0.056663</td>
      <td>0.028337</td>
      <td>0.083860</td>
      <td>-0.035216</td>
      <td>-0.088472</td>
      <td>-0.152203</td>
      <td>0.021956</td>
    </tr>
    <tr>
      <th>Dressing.1</th>
      <td>0.015275</td>
      <td>0.254722</td>
      <td>0.032265</td>
      <td>0.020850</td>
      <td>-0.020850</td>
      <td>0.049445</td>
      <td>-0.049445</td>
      <td>0.031072</td>
      <td>-0.031072</td>
      <td>0.015760</td>
      <td>...</td>
      <td>-0.016381</td>
      <td>0.094613</td>
      <td>-0.030546</td>
      <td>-0.056663</td>
      <td>-0.028337</td>
      <td>-0.083860</td>
      <td>0.035216</td>
      <td>0.088472</td>
      <td>0.152203</td>
      <td>-0.021956</td>
    </tr>
    <tr>
      <th>Doctor.0</th>
      <td>-0.037998</td>
      <td>-0.232271</td>
      <td>-0.032694</td>
      <td>-0.015150</td>
      <td>0.015150</td>
      <td>-0.063869</td>
      <td>0.063869</td>
      <td>-0.029060</td>
      <td>0.029060</td>
      <td>-0.020358</td>
      <td>...</td>
      <td>0.044375</td>
      <td>-0.060793</td>
      <td>0.034612</td>
      <td>0.057831</td>
      <td>-0.032296</td>
      <td>0.126540</td>
      <td>-0.109327</td>
      <td>-0.134708</td>
      <td>-0.204474</td>
      <td>0.022556</td>
    </tr>
    <tr>
      <th>Doctor.1</th>
      <td>0.037998</td>
      <td>0.232271</td>
      <td>0.032694</td>
      <td>0.015150</td>
      <td>-0.015150</td>
      <td>0.063869</td>
      <td>-0.063869</td>
      <td>0.029060</td>
      <td>-0.029060</td>
      <td>0.020358</td>
      <td>...</td>
      <td>-0.044375</td>
      <td>0.060793</td>
      <td>-0.034612</td>
      <td>-0.057831</td>
      <td>0.032296</td>
      <td>-0.126540</td>
      <td>0.109327</td>
      <td>0.134708</td>
      <td>0.204474</td>
      <td>-0.022556</td>
    </tr>
    <tr>
      <th>Expectation</th>
      <td>-0.010287</td>
      <td>0.027247</td>
      <td>0.060719</td>
      <td>0.004159</td>
      <td>-0.004159</td>
      <td>-0.012849</td>
      <td>0.012849</td>
      <td>0.069184</td>
      <td>-0.069184</td>
      <td>-0.024366</td>
      <td>...</td>
      <td>-0.080149</td>
      <td>0.013793</td>
      <td>-0.039769</td>
      <td>-0.208329</td>
      <td>-0.079993</td>
      <td>-0.096248</td>
      <td>0.031078</td>
      <td>0.042588</td>
      <td>-0.057544</td>
      <td>0.037979</td>
    </tr>
    <tr>
      <th>H_normal_class</th>
      <td>-0.031389</td>
      <td>0.057082</td>
      <td>0.036361</td>
      <td>-0.034942</td>
      <td>0.034942</td>
      <td>0.021900</td>
      <td>-0.021900</td>
      <td>0.005223</td>
      <td>-0.005223</td>
      <td>-0.019258</td>
      <td>...</td>
      <td>0.796782</td>
      <td>-0.079436</td>
      <td>0.182888</td>
      <td>0.052111</td>
      <td>0.047913</td>
      <td>0.057115</td>
      <td>-0.007325</td>
      <td>-0.025896</td>
      <td>-0.014458</td>
      <td>0.101990</td>
    </tr>
    <tr>
      <th>H_evac_class</th>
      <td>0.002302</td>
      <td>0.048605</td>
      <td>-0.003870</td>
      <td>-0.022613</td>
      <td>0.022613</td>
      <td>-0.015493</td>
      <td>0.015493</td>
      <td>-0.048307</td>
      <td>0.048307</td>
      <td>-0.050046</td>
      <td>...</td>
      <td>1.000000</td>
      <td>-0.043825</td>
      <td>0.232775</td>
      <td>0.128091</td>
      <td>0.051935</td>
      <td>0.139408</td>
      <td>-0.051682</td>
      <td>-0.106532</td>
      <td>-0.038651</td>
      <td>0.126627</td>
    </tr>
    <tr>
      <th>Responsible</th>
      <td>-0.121380</td>
      <td>0.018231</td>
      <td>0.056811</td>
      <td>0.043034</td>
      <td>-0.043034</td>
      <td>0.014688</td>
      <td>-0.014688</td>
      <td>0.038276</td>
      <td>-0.038276</td>
      <td>-0.070972</td>
      <td>...</td>
      <td>-0.043825</td>
      <td>1.000000</td>
      <td>0.099379</td>
      <td>-0.016255</td>
      <td>-0.120811</td>
      <td>0.048610</td>
      <td>0.127143</td>
      <td>0.114557</td>
      <td>0.063120</td>
      <td>0.053704</td>
    </tr>
    <tr>
      <th>Cost_evac_class</th>
      <td>-0.001037</td>
      <td>0.045095</td>
      <td>-0.000160</td>
      <td>0.049226</td>
      <td>-0.049226</td>
      <td>-0.071726</td>
      <td>0.071726</td>
      <td>-0.028078</td>
      <td>0.028078</td>
      <td>-0.083511</td>
      <td>...</td>
      <td>0.232775</td>
      <td>0.099379</td>
      <td>1.000000</td>
      <td>0.343876</td>
      <td>-0.084181</td>
      <td>0.024966</td>
      <td>-0.089935</td>
      <td>-0.133468</td>
      <td>-0.110666</td>
      <td>0.087106</td>
    </tr>
    <tr>
      <th>Spend_evac_class</th>
      <td>0.055481</td>
      <td>0.018576</td>
      <td>-0.026167</td>
      <td>-0.019764</td>
      <td>0.019764</td>
      <td>-0.074073</td>
      <td>0.074073</td>
      <td>-0.030821</td>
      <td>0.030821</td>
      <td>-0.108055</td>
      <td>...</td>
      <td>0.128091</td>
      <td>-0.016255</td>
      <td>0.343876</td>
      <td>1.000000</td>
      <td>0.038216</td>
      <td>0.200012</td>
      <td>-0.080005</td>
      <td>-0.159037</td>
      <td>-0.102230</td>
      <td>0.058102</td>
    </tr>
    <tr>
      <th>Age_class</th>
      <td>0.417846</td>
      <td>-0.256226</td>
      <td>-0.060611</td>
      <td>-0.025331</td>
      <td>0.025331</td>
      <td>-0.024004</td>
      <td>0.024004</td>
      <td>0.008219</td>
      <td>-0.008219</td>
      <td>0.027208</td>
      <td>...</td>
      <td>0.051935</td>
      <td>-0.120811</td>
      <td>-0.084181</td>
      <td>0.038216</td>
      <td>1.000000</td>
      <td>0.022812</td>
      <td>-0.084352</td>
      <td>-0.161845</td>
      <td>0.147961</td>
      <td>-0.112653</td>
    </tr>
    <tr>
      <th>Educate</th>
      <td>0.034905</td>
      <td>-0.050928</td>
      <td>0.012691</td>
      <td>-0.145811</td>
      <td>0.145811</td>
      <td>-0.103939</td>
      <td>0.103939</td>
      <td>-0.055927</td>
      <td>0.055927</td>
      <td>-0.062770</td>
      <td>...</td>
      <td>0.139408</td>
      <td>0.048610</td>
      <td>0.024966</td>
      <td>0.200012</td>
      <td>0.022812</td>
      <td>1.000000</td>
      <td>-0.155889</td>
      <td>-0.162265</td>
      <td>-0.146819</td>
      <td>-0.028566</td>
    </tr>
    <tr>
      <th>Dwelling.type</th>
      <td>-0.159774</td>
      <td>-0.010058</td>
      <td>-0.060854</td>
      <td>-0.068000</td>
      <td>0.068000</td>
      <td>0.048356</td>
      <td>-0.048356</td>
      <td>0.045758</td>
      <td>-0.045758</td>
      <td>0.025681</td>
      <td>...</td>
      <td>-0.051682</td>
      <td>0.127143</td>
      <td>-0.089935</td>
      <td>-0.080005</td>
      <td>-0.084352</td>
      <td>-0.155889</td>
      <td>1.000000</td>
      <td>0.404846</td>
      <td>0.096775</td>
      <td>0.081219</td>
    </tr>
    <tr>
      <th>House_ownership</th>
      <td>-0.211393</td>
      <td>-0.018509</td>
      <td>-0.098541</td>
      <td>0.019188</td>
      <td>-0.019188</td>
      <td>0.031052</td>
      <td>-0.031052</td>
      <td>0.027483</td>
      <td>-0.027483</td>
      <td>0.032009</td>
      <td>...</td>
      <td>-0.106532</td>
      <td>0.114557</td>
      <td>-0.133468</td>
      <td>-0.159037</td>
      <td>-0.161845</td>
      <td>-0.162265</td>
      <td>0.404846</td>
      <td>1.000000</td>
      <td>0.156647</td>
      <td>0.058089</td>
    </tr>
    <tr>
      <th>Employment</th>
      <td>-0.014167</td>
      <td>-0.026818</td>
      <td>0.113514</td>
      <td>-0.005478</td>
      <td>0.005478</td>
      <td>0.052428</td>
      <td>-0.052428</td>
      <td>0.036413</td>
      <td>-0.036413</td>
      <td>-0.063347</td>
      <td>...</td>
      <td>-0.038651</td>
      <td>0.063120</td>
      <td>-0.110666</td>
      <td>-0.102230</td>
      <td>0.147961</td>
      <td>-0.146819</td>
      <td>0.096775</td>
      <td>0.156647</td>
      <td>1.000000</td>
      <td>-0.006103</td>
    </tr>
    <tr>
      <th>Evac_decision</th>
      <td>-0.042199</td>
      <td>0.039622</td>
      <td>-0.027484</td>
      <td>0.054515</td>
      <td>-0.054515</td>
      <td>0.019650</td>
      <td>-0.019650</td>
      <td>0.006574</td>
      <td>-0.006574</td>
      <td>0.037263</td>
      <td>...</td>
      <td>0.126627</td>
      <td>0.053704</td>
      <td>0.087106</td>
      <td>0.058102</td>
      <td>-0.112653</td>
      <td>-0.028566</td>
      <td>0.081219</td>
      <td>0.058089</td>
      <td>-0.006103</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
<p>43 rows × 43 columns</p>
</div>




```python
# Correlation plot

import seaborn as sns
corr=df.iloc[:,:-1].corr()
top_features=corr.index
plt.figure(figsize=(20,20))
sns.heatmap(df[top_features].corr(),annot=True)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x29e0ecad630>




    
![png](output_17_1.png)
    



```python
# Define threshold

threshold=0.5
```


```python
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns
    corr_matrix = dataset.corr()
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
```


```python
correlation(df.iloc[:,:-1],threshold)
```




    {'Concentrating.1',
     'Disruption.1',
     'Doctor.0',
     'Doctor.1',
     'Dressing.1',
     'Experience.1',
     'Gender.1',
     'H_evac_class',
     'Hearing.1',
     'Injury.1',
     'Pet_Cat.1',
     'Pet_Dog.1',
     'Pet_Others.1',
     'Property_damage.1',
     'Vision.1',
     'Walking.1'}




```python
df.shape()
```


    ---------------------------------------------------------------------------

    TypeError                                 Traceback (most recent call last)

    <ipython-input-46-0e566b70f572> in <module>
    ----> 1 df.shape()
    

    TypeError: 'tuple' object is not callable



```python
df.shape
```




    (490, 43)




```python
from sklearn.feature_selection import mutual_info_classif
```


```python
mutual_info=mutual_info_classif(X,y)
```


```python
mutual_data=pd.Series(mutual_info,index=X.columns)
mutual_data.sort_values(ascending=False)
```




    Employment           0.060369
    Pet_Cat.1            0.049844
    Property_damage.1    0.037048
    Cost_evac_class      0.035561
    Concentrating.0      0.028497
    Pet_Others.1         0.027257
    Age_class            0.022806
    Experience.1         0.020709
    Pet_Dog.1            0.018632
    Hearing.1            0.014600
    Responsible          0.011520
    H_number             0.008760
    Disruption.0         0.007763
    Concentrating.1      0.004862
    Expectation          0.002558
    Doctor.1             0.000000
    Dressing.1           0.000000
    Doctor.0             0.000000
    H_evac_class         0.000000
    H_normal_class       0.000000
    Walking.1            0.000000
    Spend_evac_class     0.000000
    Educate              0.000000
    Dwelling.type        0.000000
    House_ownership      0.000000
    Dressing.0           0.000000
    Residency            0.000000
    Walking.0            0.000000
    Vision.1             0.000000
    Hearing.0            0.000000
    Gender.1             0.000000
    Gender.0             0.000000
    Pet_Others.0         0.000000
    Pet_Cat.0            0.000000
    Pet_Dog.0            0.000000
    Injury.1             0.000000
    Injury.0             0.000000
    Disruption.1         0.000000
    Property_damage.0    0.000000
    Experience.0         0.000000
    Pet_number           0.000000
    Vision.0             0.000000
    dtype: float64




```python
mutual_data
```




    Residency            0.000000
    H_number             0.008760
    Pet_number           0.000000
    Experience.0         0.000000
    Experience.1         0.020709
    Property_damage.0    0.000000
    Property_damage.1    0.037048
    Disruption.0         0.007763
    Disruption.1         0.000000
    Injury.0             0.000000
    Injury.1             0.000000
    Pet_Dog.0            0.000000
    Pet_Dog.1            0.018632
    Pet_Cat.0            0.000000
    Pet_Cat.1            0.049844
    Pet_Others.0         0.000000
    Pet_Others.1         0.027257
    Gender.0             0.000000
    Gender.1             0.000000
    Hearing.0            0.000000
    Hearing.1            0.014600
    Vision.0             0.000000
    Vision.1             0.000000
    Concentrating.0      0.028497
    Concentrating.1      0.004862
    Walking.0            0.000000
    Walking.1            0.000000
    Dressing.0           0.000000
    Dressing.1           0.000000
    Doctor.0             0.000000
    Doctor.1             0.000000
    Expectation          0.002558
    H_normal_class       0.000000
    H_evac_class         0.000000
    Responsible          0.011520
    Cost_evac_class      0.035561
    Spend_evac_class     0.000000
    Age_class            0.022806
    Educate              0.000000
    Dwelling.type        0.000000
    House_ownership      0.000000
    Employment           0.060369
    dtype: float64




```python
from sklearn.linear_model import LogisticRegression
```


```python
# all parameters not specified are set to their defaults
logisticRegr = LogisticRegression()
```


```python
logisticRegr.fit(X, y)
```

    C:\Users\Translab PC\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.
      FutureWarning)
    




    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
              intercept_scaling=1, max_iter=100, multi_class='warn',
              n_jobs=None, penalty='l2', random_state=None, solver='warn',
              tol=0.0001, verbose=0, warm_start=False)




```python
predictions = logisticRegr.predict(X)
```

    C:\Users\Translab PC\Anaconda3\lib\site-packages\sklearn\linear_model\base.py:283: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      indices = (scores > 0).astype(np.int)
    


```python
score = logisticRegr.score(X, y)
print(score)
```

    0.7428571428571429
    

    C:\Users\Translab PC\Anaconda3\lib\site-packages\sklearn\linear_model\base.py:283: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      indices = (scores > 0).astype(np.int)
    


```python
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import metrics
```


```python
cm = metrics.confusion_matrix(y, predictions)
print(cm)
```

    [[356  10]
     [116   8]]
    


```python
import statsmodels.api as sm
logit_model=sm.Logit(y,X)
result=logit_model.fit()
print(result.summary2())
```

    Warning: Maximum number of iterations has been exceeded.
             Current function value: 0.525835
             Iterations: 35
                                                Results: Logit
    ======================================================================================================
    Model:                           Logit                         Pseudo R-squared:              0.070   
    Dependent Variable:              Evac_decision                 AIC:                           573.3184
    Date:                            2022-03-16 13:47              BIC:                           694.9561
    No. Observations:                490                           Log-Likelihood:                -257.66 
    Df Model:                        28                            LL-Null:                       -277.18 
    Df Residuals:                    461                           LLR p-value:                   0.080258
    Converged:                       0.0000                        Scale:                         1.0000  
    No. Iterations:                  35.0000                                                              
    ------------------------------------------------------------------------------------------------------
                       Coef.         Std.Err.         z    P>|z|         [0.025               0.975]      
    ------------------------------------------------------------------------------------------------------
    Residency           0.0063              0.0065  0.9723 0.3309              -0.0064              0.0190
    H_number            0.0107              0.0716  0.1501 0.8807              -0.1296              0.1511
    Pet_number         -0.0726              0.0691 -1.0506 0.2935              -0.2080              0.0628
    Experience.0      -11.6799                 nan     nan    nan                  nan                 nan
    Experience.1      -11.9965                 nan     nan    nan                  nan                 nan
    Property_damage.0  63.3976        3869757.5365  0.0000 1.0000        -7584522.0029        7584648.7981
    Property_damage.1  63.2705        3920610.6707  0.0000 1.0000        -7684192.4416        7684318.9825
    Disruption.0      -15.3459                 nan     nan    nan                  nan                 nan
    Disruption.1      -15.2144                 nan     nan    nan                  nan                 nan
    Injury.0           44.9404                 nan     nan    nan                  nan                 nan
    Injury.1          -16.0927 14558690795733.7324 -0.0000 1.0000 -28534509621708.9883 28534509621676.8008
    Pet_Dog.0          -9.3917                 nan     nan    nan                  nan                 nan
    Pet_Dog.1          -9.2658                 nan     nan    nan                  nan                 nan
    Pet_Cat.0         -11.2311                 nan     nan    nan                  nan                 nan
    Pet_Cat.1         -11.3946                 nan     nan    nan                  nan                 nan
    Pet_Others.0       -7.0590        3855449.2042 -0.0000 1.0000        -7556548.6435        7556534.5254
    Pet_Others.1       -6.4328        3713021.6094 -0.0000 1.0000        -7277395.0610        7277382.1954
    Gender.0          -11.3024                 nan     nan    nan                  nan                 nan
    Gender.1          -11.6037                 nan     nan    nan                  nan                 nan
    Hearing.0         -17.0545                 nan     nan    nan                  nan                 nan
    Hearing.1         -16.5794                 nan     nan    nan                  nan                 nan
    Vision.0           -5.7132                 nan     nan    nan                  nan                 nan
    Vision.1           -5.2711                 nan     nan    nan                  nan                 nan
    Concentrating.0   -12.1640        3377801.3014 -0.0000 1.0000        -6620381.0616        6620356.7336
    Concentrating.1   -12.0930        3377801.3014 -0.0000 1.0000        -6620380.9907        6620356.8046
    Walking.0          -4.7060        2353333.4880 -0.0000 1.0000        -4612453.5861        4612444.1740
    Walking.1          -5.3643        2365818.1034 -0.0000 1.0000        -4636923.6409        4636912.9123
    Dressing.0         -2.2837                 nan     nan    nan                  nan                 nan
    Dressing.1         -2.3859                 nan     nan    nan                  nan                 nan
    Doctor.0           -2.2560        2261453.1042 -0.0000 1.0000        -4432368.8930        4432364.3811
    Doctor.1           -2.3608        2261453.1042 -0.0000 1.0000        -4432368.9978        4432364.2763
    Expectation         0.2077              0.1798  1.1554 0.2479              -0.1446              0.5601
    H_normal_class      0.0045              0.1021  0.0438 0.9651              -0.1956              0.2046
    H_evac_class        0.1488              0.0834  1.7837 0.0745              -0.0147              0.3122
    Responsible         0.1358              0.1684  0.8064 0.4200              -0.1942              0.4658
    Cost_evac_class     0.0528              0.0979  0.5389 0.5900              -0.1392              0.2447
    Spend_evac_class    0.1361              0.1092  1.2467 0.2125              -0.0779              0.3502
    Age_class          -0.1616              0.0896 -1.8028 0.0714              -0.3373              0.0141
    Educate            -0.0277              0.0539 -0.5148 0.6067              -0.1334              0.0779
    Dwelling.type       0.2371              0.1742  1.3607 0.1736              -0.1044              0.5786
    House_ownership     0.1469              0.2741  0.5359 0.5920              -0.3903              0.6840
    Employment          0.0856              0.0922  0.9284 0.3532              -0.0952              0.2664
    ======================================================================================================
    
    

    C:\Users\Translab PC\Anaconda3\lib\site-packages\statsmodels\base\model.py:568: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals
      "Check mle_retvals", ConvergenceWarning)
    C:\Users\Translab PC\Anaconda3\lib\site-packages\statsmodels\base\model.py:1354: RuntimeWarning: invalid value encountered in sqrt
      bse_ = np.sqrt(np.diag(self.cov_params()))
    


```python

```
